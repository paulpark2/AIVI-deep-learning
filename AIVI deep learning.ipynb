{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98b7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI006_IK15HGAE_data_PCON\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Setup train, validation and test generators\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    os.path.join(train_dir, \"Cam3P1L2\\\\obj2\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    os.path.join(train_dir, \"Cam3P1L2\\\\obj2\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    os.path.join(test_dir, \"Cam3P1L2\\\\obj2\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the ResNet50 model, pre-trained on ImageNet\n",
    "base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "\n",
    "# Freeze the base model layers during the initial training phase.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Custom layers on top of the base model\n",
    "inputs = Input(shape=input_shape)\n",
    "x = base_model(inputs, training=False)\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation='softmax')(x)  # 10 units for 10 different defect types\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=10,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2190e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 380 images belonging to 2 classes.\n",
      "Found 94 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "24/24 [==============================] - 90s 3s/step - loss: 0.7691 - accuracy: 0.4974 - val_loss: 0.7553 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - 76s 3s/step - loss: 0.7301 - accuracy: 0.4974 - val_loss: 0.6959 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - 81s 3s/step - loss: 0.6971 - accuracy: 0.5500 - val_loss: 0.6942 - val_accuracy: 0.4362\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - 80s 3s/step - loss: 0.7091 - accuracy: 0.5263 - val_loss: 0.7017 - val_accuracy: 0.4043\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - 79s 3s/step - loss: 0.6838 - accuracy: 0.5553 - val_loss: 0.7073 - val_accuracy: 0.5000\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 75s 3s/step - loss: 0.7161 - accuracy: 0.5158 - val_loss: 0.7089 - val_accuracy: 0.5000\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - 77s 3s/step - loss: 0.6944 - accuracy: 0.5158 - val_loss: 0.7436 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 78s 3s/step - loss: 0.6823 - accuracy: 0.5684 - val_loss: 0.7533 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 77s 3s/step - loss: 0.6889 - accuracy: 0.5474 - val_loss: 0.7303 - val_accuracy: 0.4255\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 77s 3s/step - loss: 0.6771 - accuracy: 0.5868 - val_loss: 0.8225 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - 81s 3s/step - loss: 0.6742 - accuracy: 0.6026 - val_loss: 0.8978 - val_accuracy: 0.5000\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 77s 3s/step - loss: 0.7069 - accuracy: 0.5632 - val_loss: 0.8707 - val_accuracy: 0.5000\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 71s 3s/step - loss: 0.6960 - accuracy: 0.5421 - val_loss: 0.8292 - val_accuracy: 0.4787\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 72s 3s/step - loss: 0.6966 - accuracy: 0.5421 - val_loss: 0.7692 - val_accuracy: 0.3936\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 75s 3s/step - loss: 0.7009 - accuracy: 0.5842 - val_loss: 0.7678 - val_accuracy: 0.3936\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 227s 10s/step - loss: 0.7013 - accuracy: 0.5526 - val_loss: 0.8019 - val_accuracy: 0.3191\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 90s 4s/step - loss: 0.6761 - accuracy: 0.6000 - val_loss: 0.7783 - val_accuracy: 0.3191\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 76s 3s/step - loss: 0.6763 - accuracy: 0.5974 - val_loss: 0.8501 - val_accuracy: 0.3085\n",
      "Epoch 19/30\n",
      "24/24 [==============================] - 80s 3s/step - loss: 0.6743 - accuracy: 0.5816 - val_loss: 0.9058 - val_accuracy: 0.4468\n",
      "Epoch 20/30\n",
      "24/24 [==============================] - 88s 4s/step - loss: 0.6843 - accuracy: 0.5658 - val_loss: 1.1550 - val_accuracy: 0.5000\n",
      "Epoch 21/30\n",
      "24/24 [==============================] - 86s 4s/step - loss: 0.6408 - accuracy: 0.6237 - val_loss: 0.9599 - val_accuracy: 0.4255\n",
      "Epoch 22/30\n",
      "24/24 [==============================] - 94s 4s/step - loss: 0.6887 - accuracy: 0.5842 - val_loss: 0.8753 - val_accuracy: 0.3511\n",
      "Epoch 23/30\n",
      "24/24 [==============================] - 83s 3s/step - loss: 0.6789 - accuracy: 0.5763 - val_loss: 0.8566 - val_accuracy: 0.4362\n",
      "Epoch 24/30\n",
      "24/24 [==============================] - 90s 4s/step - loss: 0.6961 - accuracy: 0.5684 - val_loss: 0.9167 - val_accuracy: 0.2766\n",
      "Epoch 25/30\n",
      "24/24 [==============================] - 81s 3s/step - loss: 0.6821 - accuracy: 0.6079 - val_loss: 1.2048 - val_accuracy: 0.4681\n",
      "Epoch 26/30\n",
      "24/24 [==============================] - 92s 4s/step - loss: 0.6690 - accuracy: 0.6079 - val_loss: 1.0257 - val_accuracy: 0.4149\n",
      "Epoch 27/30\n",
      "24/24 [==============================] - 81s 3s/step - loss: 0.6696 - accuracy: 0.6000 - val_loss: 1.2607 - val_accuracy: 0.5000\n",
      "Epoch 28/30\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.6778 - accuracy: 0.5921 - val_loss: 0.9755 - val_accuracy: 0.2447\n",
      "Epoch 29/30\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.6642 - accuracy: 0.6026 - val_loss: 1.0972 - val_accuracy: 0.4149\n",
      "Epoch 30/30\n",
      "24/24 [==============================] - 83s 3s/step - loss: 0.6904 - accuracy: 0.5816 - val_loss: 0.9390 - val_accuracy: 0.2660\n"
     ]
    }
   ],
   "source": [
    "#ResNet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI005_6_IK15HGAE_data_PCBCAM3\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "#test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Setup train, validation and test generators\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#test_set = test_datagen.flow_from_directory(\n",
    "#    test_dir,\n",
    "#    target_size=(img_size, img_size),\n",
    "#    batch_size=batch_size,\n",
    "#    class_mode='binary',\n",
    "#    shuffle=False\n",
    "#)\n",
    "\n",
    "# Load the ResNet50 model, pre-trained on ImageNet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=tf.keras.Input(shape=(img_size, img_size, 3)))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers[:-10]: #\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the top layers to be trained with our data\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x) #added\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "#test_loss, test_acc = model.evaluate(test_set)\n",
    "#print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d01831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"AIVI005_6_PCB.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10740025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI006_IK15HGAE_data_ALL\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Setup train, validation and test generators\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the ResNet50 model, pre-trained on ImageNet\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the top layers to be trained with our data\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=50,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22c72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f28badd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 273 images belonging to 2 classes.\n",
      "Found 67 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "18/18 [==============================] - 62s 3s/step - loss: 0.7008 - accuracy: 0.4652 - val_loss: 0.6923 - val_accuracy: 0.5224\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 50s 3s/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6943 - val_accuracy: 0.4776\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.6947 - accuracy: 0.4725 - val_loss: 0.6928 - val_accuracy: 0.5224\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 50s 3s/step - loss: 0.6936 - accuracy: 0.5275 - val_loss: 0.6936 - val_accuracy: 0.4776\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 49s 3s/step - loss: 0.6936 - accuracy: 0.5165 - val_loss: 0.6920 - val_accuracy: 0.5224\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.6921 - accuracy: 0.5165 - val_loss: 0.6955 - val_accuracy: 0.5224\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.6988 - accuracy: 0.5092 - val_loss: 0.6923 - val_accuracy: 0.5224\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.6962 - accuracy: 0.5201 - val_loss: 0.6922 - val_accuracy: 0.5224\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 49s 3s/step - loss: 0.6937 - accuracy: 0.5201 - val_loss: 0.6922 - val_accuracy: 0.5224\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.6931 - accuracy: 0.5201 - val_loss: 0.6927 - val_accuracy: 0.5224\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.6930 - accuracy: 0.5201 - val_loss: 0.6920 - val_accuracy: 0.5224\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 48s 3s/step - loss: 0.6965 - accuracy: 0.5201 - val_loss: 0.6923 - val_accuracy: 0.5224\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.6940 - accuracy: 0.4982 - val_loss: 0.6918 - val_accuracy: 0.5224\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.5201"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22684\\74373034.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m                         )\n\u001b[1;32m-> 1729\u001b[1;33m                     val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2070\u001b[0m                         ):\n\u001b[0;32m   2071\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2072\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2073\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2074\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1756\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1757\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#AlexNet\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI006_IK15HGAE_data_PCBCAM3\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Define the class names to be predicted\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_set = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    MaxPooling2D(3, strides=(2,2)),\n",
    "    Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "    MaxPooling2D(3, strides=(2,2)),\n",
    "    Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "    Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(3, strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0cc412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6081284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29de5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248 images belonging to 2 classes.\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.9087 - accuracy: 0.6169\n",
      "Test accuracy: 0.6169354915618896\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_dir = \"D:\\\\01.Paul\\\\overall_IK15HGAE_data2\\\\test\\\\Cam3P1L1\\\\obj2\"\n",
    "\n",
    "# Define the data generator for the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate batches of test data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10670b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f0eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 238 images belonging to 2 classes.\n",
      "Found 59 images belonging to 2 classes.\n",
      "Found 297 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "15/15 [==============================] - 792s 53s/step - loss: 4.0831 - accuracy: 0.7226 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 882s 59s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 824s 55s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 802s 54s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 769s 51s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 691s 46s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 630s 42s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 664s 44s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 760s 51s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 765s 51s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 738s 50s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 739s 49s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 759s 51s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 747s 50s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 752s 50s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 746s 50s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 733s 49s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 762s 51s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 699s 47s/step - loss: 4.2288 - accuracy: 0.7227 - val_loss: 4.1354 - val_accuracy: 0.7288\n",
      "Epoch 20/30\n"
     ]
    }
   ],
   "source": [
    "#Fully Convolutional Networks (FCNs)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI005_IK15HGA\\\\data_5_15HE\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "\n",
    "class_names = [\"Cam3P1L1\\\\obj2\", \"Cam3P1L1\\\\obj2\"]\n",
    "img_size = 256\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    os.path.join(train_dir, class_names[0]),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    os.path.join(train_dir, class_names[0]),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_set = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    os.path.join(train_dir, class_names[1]),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(img_size, img_size, 3)),\n",
    "    Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(4096, kernel_size=(7, 7), activation='relu', padding='valid'),\n",
    "    Conv2D(4096, kernel_size=(1, 1), activation='relu', padding='valid'),\n",
    "    Conv2D(1, kernel_size=(1, 1), activation='sigmoid', padding='same'),\n",
    "    UpSampling2D(size=(32, 32), interpolation='bilinear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc451841",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(test_good_dir, \"example.jpg\")\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (img_size, img_size))\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prob_map = model.predict(img)[0,:,:,1]  # probability map for the defective class\n",
    "prob_map = cv2.resize(prob_map, (img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "height, width = img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b96de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"D:\\\\01.Paul\\\\AIVI_image_IK15HGA_KUUE00E\\\\data\\\\test\\\\good\\\\Cam1P1L1\\\\obj2\\\\Frame001B1Row1Col02Obj02.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c12545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aeb02f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#Set up the data directory path containing images and labels\n",
    "\n",
    "data_dir = \"D:\\01.Paul\\AIVI_image_IK15HGA_KUUE00E\\data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "\n",
    "#Set up the file paths for good and defective training images\n",
    "\n",
    "train_good_dir = os.path.join(train_dir, \"good\\Cam3P1L1\\obj2\")\n",
    "train_defective_dir = os.path.join(train_dir, \"defective\\Cam3P1L1\\obj2\")\n",
    "\n",
    "\n",
    "#Set up the file paths for good and defective test images\n",
    "\n",
    "test_good_dir = os.path.join(test_dir, \"good\\Cam3P1L1\\obj3\")\n",
    "test_defective_dir = os.path.join(test_dir, \"defective\\Cam3P1L1\\obj2\")\n",
    "\n",
    "\n",
    "#Define the class names to be predicted\n",
    "\n",
    "class_names = [\"good\\Cam3P1L1\\obj2\", \"defective\\Cam3P1L1\\obj2\"]\n",
    "\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#Define data augmentation techniques\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "\n",
    "test_set = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "#Define a function to extract patches from an image\n",
    "\n",
    "def extract_patches(img, patch_size=(64, 64)):\n",
    "    height, width, _ = img.shape\n",
    "    patches = []\n",
    "    for i in range(0, height - patch_size[0], patch_size[0]):\n",
    "        for j in range(0, width - patch_size[1], patch_size[1]):\n",
    "            patch = img[i:i+patch_size[0], j:j+patch_size[1], :]\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    MaxPooling2D(3, strides=(2,2)),\n",
    "    Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "    MaxPooling2D(3, strides=(2,2)),\n",
    "    Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "    Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(3, strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#Compile the model with categorical cross-entropy loss\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Train the model\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate the model on test data\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "#Extract patches from a test image and make predictions on each patch\n",
    "\n",
    "img_path = os.path.join(test_good_dir, \"example.jpg\")\n",
    "img = cv2.imread(img_path)\n",
    "patches = extract_patches(img, patch_size=(224,224))\n",
    "for patch in patches:\n",
    "    patch = cv2.resize(patch, (img_size, img_size))\n",
    "    patch = np.expand_dims(patch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de29743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 9s 2s/step - loss: 445.6335 - accuracy: 0.5116 - val_loss: 536.2238 - val_accuracy: 0.7442\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 614.6183 - accuracy: 0.7442 - val_loss: 232.7298 - val_accuracy: 0.2558\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 272.1339 - accuracy: 0.6047 - val_loss: 70.3107 - val_accuracy: 0.2558\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 160.6111 - accuracy: 0.5814 - val_loss: 10.2774 - val_accuracy: 0.7442\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 94.0721 - accuracy: 0.5814 - val_loss: 2.3344 - val_accuracy: 0.4186\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 105.8957 - accuracy: 0.5581 - val_loss: 39.9451 - val_accuracy: 0.7442\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 78.2931 - accuracy: 0.6977 - val_loss: 4.6506 - val_accuracy: 0.7442\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 43.1901 - accuracy: 0.6512 - val_loss: 33.6797 - val_accuracy: 0.2558\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 57.6924 - accuracy: 0.5814 - val_loss: 18.5025 - val_accuracy: 0.2558\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 64.2224 - accuracy: 0.5349 - val_loss: 6.4380 - val_accuracy: 0.7442\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 25.3544 - accuracy: 0.6279 - val_loss: 0.8813 - val_accuracy: 0.7674\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 21.4347 - accuracy: 0.5814 - val_loss: 1.6204 - val_accuracy: 0.7442\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 18.2303 - accuracy: 0.6744 - val_loss: 12.8104 - val_accuracy: 0.2558\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 31.8285 - accuracy: 0.4419 - val_loss: 1.7424 - val_accuracy: 0.7442\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 7s 3s/step - loss: 8.5370 - accuracy: 0.7209 - val_loss: 1.0590 - val_accuracy: 0.7442\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 7s 2s/step - loss: 11.9299 - accuracy: 0.5814 - val_loss: 2.0235 - val_accuracy: 0.7442\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 6s 2s/step - loss: 10.8739 - accuracy: 0.6744 - val_loss: 0.5612 - val_accuracy: 0.6047\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 7s 2s/step - loss: 7.0806 - accuracy: 0.5349 - val_loss: 1.0751 - val_accuracy: 0.3023\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 8s 3s/step - loss: 6.9319 - accuracy: 0.6047 - val_loss: 0.6479 - val_accuracy: 0.7442\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 9s 3s/step - loss: 5.4172 - accuracy: 0.6977 - val_loss: 1.9074 - val_accuracy: 0.2558\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 8s 3s/step - loss: 4.3676 - accuracy: 0.6279 - val_loss: 0.6021 - val_accuracy: 0.7442\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 8s 3s/step - loss: 4.3048 - accuracy: 0.6512 - val_loss: 0.5349 - val_accuracy: 0.7442\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 7s 2s/step - loss: 2.7538 - accuracy: 0.6512 - val_loss: 0.5530 - val_accuracy: 0.7907\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.2498 - accuracy: 0.7209 - val_loss: 0.5344 - val_accuracy: 0.7907\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 8s 3s/step - loss: 3.2265 - accuracy: 0.5581 - val_loss: 0.5366 - val_accuracy: 0.7442\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 7s 2s/step - loss: 2.1453 - accuracy: 0.6047 - val_loss: 0.5387 - val_accuracy: 0.7209\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.1077 - accuracy: 0.6279 - val_loss: 0.5262 - val_accuracy: 0.7674\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.4531 - accuracy: 0.6977 - val_loss: 0.5405 - val_accuracy: 0.7907\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 10s 3s/step - loss: 2.3770 - accuracy: 0.5814 - val_loss: 0.5017 - val_accuracy: 0.7442\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 7s 2s/step - loss: 1.5224 - accuracy: 0.7442 - val_loss: 0.5688 - val_accuracy: 0.8372\n",
      "4/4 [==============================] - 2s 446ms/step - loss: 0.6628 - accuracy: 0.6852\n",
      "Test accuracy: 0.6851851940155029\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI_image_IK15HGA_KUUE00E\\\\data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Set up the file paths for good and defective training images\n",
    "train_good_dir = os.path.join(train_dir, \"good\\\\Cam3P1L1\\\\obj2\")\n",
    "train_defective_dir = os.path.join(train_dir, \"defective\\\\Cam3P1L1\\\\obj2\")\n",
    "\n",
    "# Set up the file paths for good and defective test images\n",
    "test_good_dir = os.path.join(test_dir, \"good\\\\Cam3P1L1\\\\obj3\")\n",
    "test_defective_dir = os.path.join(test_dir, \"defective\\\\Cam3P1L1\\\\obj2\")\n",
    "\n",
    "# Define the class names to be predicted\n",
    "class_names = [\"good\\\\Cam3P1L1\\\\obj2\", \"defective\\\\Cam3P1L1\\\\obj2\"]\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "train_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "train_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "train_imgs = train_good_imgs + train_defective_imgs\n",
    "train_labels = [0] * len(train_good_imgs) + [1] * len(train_defective_imgs)\n",
    "train_labels = to_categorical(train_labels, num_classes=2) # convert to categorical labels\n",
    "train_set = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels))\n",
    "train_set = train_set.shuffle(len(train_imgs))\n",
    "train_set = train_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "train_set = train_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "train_set = train_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "train_set = train_set.batch(batch_size)\n",
    "\n",
    "val_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "val_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "val_imgs = val_good_imgs + val_defective_imgs\n",
    "val_labels = [0] * len(val_good_imgs) + [1] * len(val_defective_imgs)\n",
    "val_labels = to_categorical(val_labels, num_classes=2) # convert to categorical labels\n",
    "val_set = tf.data.Dataset.from_tensor_slices((val_imgs, val_labels))\n",
    "val_set = val_set.shuffle(len(val_imgs))\n",
    "val_set = val_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "val_set = val_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "val_set = val_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "val_set = val_set.batch(batch_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_good_imgs = [os.path.join(test_good_dir, f) for f in os.listdir(test_good_dir) if os.path.isfile(os.path.join(test_good_dir, f))]\n",
    "test_defective_imgs = [os.path.join(test_defective_dir, f) for f in os.listdir(test_defective_dir) if os.path.isfile(os.path.join(test_defective_dir, f))]\n",
    "test_imgs = test_good_imgs + test_defective_imgs\n",
    "test_labels = [0] * len(test_good_imgs) + [1] * len(test_defective_imgs)\n",
    "test_labels = to_categorical(test_labels, num_classes=2)\n",
    "test_set = tf.data.Dataset.from_tensor_slices((test_imgs, test_labels))\n",
    "test_set = test_set.shuffle(len(test_imgs))\n",
    "test_set = test_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "test_set = test_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "test_set = test_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "test_set = test_set.batch(batch_size)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582b041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c881f2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb040ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI_image_IK15HGA_KUUE00E\\\\data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Set up the file paths for good and defective training images\n",
    "train_good_dir = os.path.join(train_dir, \"good\\\\Cam1P1L1\\\\obj4\")\n",
    "train_defective_dir = os.path.join(train_dir, \"defective\\\\Cam1P1L1\\\\obj4\")\n",
    "\n",
    "# Set up the file paths for good and defective test images\n",
    "test_good_dir = os.path.join(test_dir, \"good\\\\Cam1P1L1\\\\obj4\")\n",
    "test_defective_dir = os.path.join(test_dir, \"defective\\\\Cam1P1L1\\\\obj4\")\n",
    "\n",
    "# Define the class names to be predicted\n",
    "class_names = [\"good\\\\Cam1P1L1\\\\obj4\", \"defective\\\\Cam1P1L1\\\\obj4\"]\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "train_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "train_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "train_imgs = train_good_imgs + train_defective_imgs\n",
    "train_labels = [0] * len(train_good_imgs) + [1] * len(train_defective_imgs)\n",
    "train_labels = to_categorical(train_labels, num_classes=2) # convert to categorical labels\n",
    "train_set = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels))\n",
    "train_set = train_set.shuffle(len(train_imgs))\n",
    "train_set = train_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "train_set = train_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "train_set = train_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "train_set = train_set.batch(batch_size)\n",
    "\n",
    "val_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "val_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "val_imgs = val_good_imgs + val_defective_imgs\n",
    "val_labels = [0] * len(val_good_imgs) + [1] * len(val_defective_imgs)\n",
    "val_labels = to_categorical(val_labels, num_classes=2) # convert to categorical labels\n",
    "val_set = tf.data.Dataset.from_tensor_slices((val_imgs, val_labels))\n",
    "val_set = val_set.shuffle(len(val_imgs))\n",
    "val_set = val_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "val_set = val_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "val_set = val_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "val_set = val_set.batch(batch_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_good_imgs = [os.path.join(test_good_dir, f) for f in os.listdir(test_good_dir) if os.path.isfile(os.path.join(test_good_dir, f))]\n",
    "test_defective_imgs = [os.path.join(test_defective_dir, f) for f in os.listdir(test_defective_dir) if os.path.isfile(os.path.join(test_defective_dir, f))]\n",
    "test_imgs = test_good_imgs + test_defective_imgs\n",
    "test_labels = [0] * len(test_good_imgs) + [1] * len(test_defective_imgs)\n",
    "test_labels = to_categorical(test_labels, num_classes=2)\n",
    "test_set = tf.data.Dataset.from_tensor_slices((test_imgs, test_labels))\n",
    "test_set = test_set.shuffle(len(test_imgs))\n",
    "test_set = test_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "test_set = test_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "test_set = test_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "test_set = test_set.batch(batch_size)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee689b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set up the data directory path containing images and labels\n",
    "data_dir = \"D:\\\\01.Paul\\\\AIVI_image_IK15HGA_KUUE00E\\\\data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Set up the file paths for good and defective training images\n",
    "train_good_dir = os.path.join(train_dir, \"good\\\\Cam1P1L1\\\\obj4\")\n",
    "train_defective_dir = os.path.join(train_dir, \"defective\\\\Cam1P1L1\\\\obj4\")\n",
    "\n",
    "# Set up the file paths for good and defective test images\n",
    "test_good_dir = os.path.join(test_dir, \"good\\\\Cam1P1L1\\\\obj4\")\n",
    "test_defective_dir = os.path.join(test_dir, \"defective\\\\Cam1P1L1\\\\obj4\")\n",
    "\n",
    "# Define the class names to be predicted\n",
    "class_names = [\"good\\\\Cam1P1L1\\\\obj4\", \"defective\\\\Cam1P1L1\\\\obj4\"]\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "# Define data augmentation techniques\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "train_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "train_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "train_imgs = train_good_imgs + train_defective_imgs\n",
    "train_labels = [0] * len(train_good_imgs) + [1] * len(train_defective_imgs)\n",
    "train_labels = to_categorical(train_labels, num_classes=2) # convert to categorical labels\n",
    "train_set = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels))\n",
    "train_set = train_set.shuffle(len(train_imgs))\n",
    "train_set = train_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "train_set = train_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "train_set = train_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "train_set = train_set.batch(batch_size)\n",
    "\n",
    "val_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "val_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "val_imgs = val_good_imgs + val_defective_imgs\n",
    "val_labels = [0] * len(val_good_imgs) + [1] * len(val_defective_imgs)\n",
    "val_labels = to_categorical(val_labels, num_classes=2) # convert to categorical labels\n",
    "val_set = tf.data.Dataset.from_tensor_slices((val_imgs, val_labels))\n",
    "val_set = val_set.shuffle(len(val_imgs))\n",
    "val_set = val_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "val_set = val_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "val_set = val_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "val_set = val_set.batch(batch_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=30,\n",
    "    validation_data=val_set\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_good_imgs = [os.path.join(test_good_dir, f) for f in os.listdir(test_good_dir) if os.path.isfile(os.path.join(test_good_dir, f))]\n",
    "test_defective_imgs = [os.path.join(test_defective_dir, f) for f in os.listdir(test_defective_dir) if os.path.isfile(os.path.join(test_defective_dir, f))]\n",
    "test_imgs = test_good_imgs + test_defective_imgs\n",
    "test_labels = [0] * len(test_good_imgs) + [1] * len(test_defective_imgs)\n",
    "test_labels = to_categorical(test_labels, num_classes=2)\n",
    "test_set = tf.data.Dataset.from_tensor_slices((test_imgs, test_labels))\n",
    "test_set = test_set.shuffle(len(test_imgs))\n",
    "test_set = test_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "test_set = test_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "test_set = test_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "test_set = test_set.batch(batch_size)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd23e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4052 images belonging to 2 classes.\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        for k in range(3):\n",
    "            # Set up the data directory path containing images and labels\n",
    "            data_dir = \"D:\\\\01.Paul\\\\AIVI_image_IK15HGA_KUUE00E\\\\data\"\n",
    "            train_dir = os.path.join(data_dir, \"train\")\n",
    "            test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "            # Set up the file paths for good and defective training images\n",
    "            train_good_dir = os.path.join(train_dir, \"good\\\\Cam1P1L\"+str(j)+\"\\\\obj\" + str(k))\n",
    "            train_defective_dir = os.path.join(train_dir, \"defective\\\\Cam1P1L\"+str(j)+\"\\\\obj\" + str(k))\n",
    "\n",
    "            # Set up the file paths for good and defective test images\n",
    "            test_good_dir = os.path.join(test_dir, \"good\\\\Cam1P1L1\\\\obj\" + str(k))\n",
    "            test_defective_dir = os.path.join(test_dir, \"defective\\\\Cam1P1L\"+str(j)+\"\\\\obj\" + str(k))\n",
    "\n",
    "            # Define the class names to be predicted\n",
    "            class_names = [\"good\\\\Cam1P1L1\\\\obj\" + str(k), \"defective\\\\Cam1P1L\"+str(j)+\"\\\\obj\" + str(k)]\n",
    "\n",
    "            img_size = 24\n",
    "            batch_size = 16\n",
    "\n",
    "            # Define data augmentation techniques\n",
    "            train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1./255,\n",
    "                rotation_range=40,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                zoom_range=0.2,\n",
    "                validation_split=0.2,\n",
    "            )\n",
    "\n",
    "            train_set = train_datagen.flow_from_directory(\n",
    "                train_dir,\n",
    "                target_size=(img_size, img_size),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='categorical',\n",
    "                subset='training'\n",
    "            )\n",
    "\n",
    "            train_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "            train_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "            train_imgs = train_good_imgs + train_defective_imgs\n",
    "            train_labels = [0] * len(train_good_imgs) + [1] * len(train_defective_imgs)\n",
    "            train_labels = to_categorical(train_labels, num_classes=2) # convert to categorical labels\n",
    "            train_set = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels))\n",
    "            train_set = train_set.shuffle(len(train_imgs))\n",
    "            train_set = train_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "            train_set = train_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "            train_set = train_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "            train_set = train_set.batch(batch_size)\n",
    "\n",
    "            val_good_imgs = [os.path.join(train_good_dir, f) for f in os.listdir(train_good_dir) if os.path.isfile(os.path.join(train_good_dir, f))]\n",
    "            val_defective_imgs = [os.path.join(train_defective_dir, f) for f in os.listdir(train_defective_dir) if os.path.isfile(os.path.join(train_defective_dir, f))]\n",
    "            val_imgs = val_good_imgs + val_defective_imgs\n",
    "            val_labels = [0] * len(val_good_imgs) + [1] * len(val_defective_imgs)\n",
    "            val_labels = to_categorical(val_labels, num_classes=2) # convert to categorical labels\n",
    "            val_set = tf.data.Dataset.from_tensor_slices((val_imgs, val_labels))\n",
    "            val_set = val_set.shuffle(len(val_imgs))\n",
    "            val_set = val_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "            val_set = val_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "            val_set = val_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "            val_set = val_set.batch(batch_size)\n",
    "\n",
    "            model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "                tf.keras.layers.MaxPooling2D(2,2),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D(2,2),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D(2,2),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(512, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(2, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the model with categorical cross-entropy loss\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            # Train the model\n",
    "            history = model.fit(\n",
    "                train_set,\n",
    "                epochs=30,\n",
    "                validation_data=val_set\n",
    "            )\n",
    "\n",
    "            # Evaluate the model on test data\n",
    "            test_good_imgs = [os.path.join(test_good_dir, f) for f in os.listdir(test_good_dir) if os.path.isfile(os.path.join(test_good_dir, f))]\n",
    "            test_defective_imgs = [os.path.join(test_defective_dir, f) for f in os.listdir(test_defective_dir) if os.path.isfile(os.path.join(test_defective_dir, f))]\n",
    "            test_imgs = test_good_imgs + test_defective_imgs\n",
    "            test_labels = [0] * len(test_good_imgs) + [1] * len(test_defective_imgs)\n",
    "            test_labels = to_categorical(test_labels, num_classes=2)\n",
    "            test_set = tf.data.Dataset.from_tensor_slices((test_imgs, test_labels))\n",
    "            test_set = test_set.shuffle(len(test_imgs))\n",
    "            test_set = test_set.map(lambda x, y: (tf.io.read_file(x), y))\n",
    "            test_set = test_set.map(lambda x, y: (tf.image.decode_jpeg(x, channels=3), y))\n",
    "            test_set = test_set.map(lambda x, y: (tf.image.resize(x, (img_size, img_size)), y))\n",
    "            test_set = test_set.batch(batch_size)\n",
    "\n",
    "            test_loss, test_acc = model.evaluate(test_set)\n",
    "            print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
